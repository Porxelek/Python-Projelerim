{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Porxelek/Python-Projelerim/blob/main/aims_task_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uITZdg-tJ4vE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from wordcloud import WordCloud\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, recall_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from yellowbrick.classifier import PrecisionRecallCurve\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
        "pd.set_option('display.max_columns', 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# burda header ile ilgili duzenleme yapildi cunku data duzgun okunmuyordu "
      ],
      "metadata": {
        "id": "MDVyIJ7MTSxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CdvjM3Z90saV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df0 = pd.read_csv('/content/drive/MyDrive/Data/Comments_Demo.csv')"
      ],
      "metadata": {
        "id": "RQokzerYSCsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df0.copy()\n",
        "df.sample(13)"
      ],
      "metadata": {
        "id": "4g-lZhMiKool"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eUoOyNG7Ynio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_sep(x):\n",
        "  if x.__contains__(\",\"):\n",
        "      return x.split(\",\")[0]\n",
        "  elif x.__contains__(\"\\|\") :\n",
        "      return x.split(\"\\|\")[0]\n",
        "df[\"id\"] = df[\"ID,Comments\"].apply(split_sep)"
      ],
      "metadata": {
        "id": "sADw7inpUUTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"id\"]"
      ],
      "metadata": {
        "id": "myOzPPNo2HpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sep1(x):\n",
        "  if x.__contains__(\",\"):\n",
        "      return x.split(\",\")[1]\n",
        "  elif x.__contains__(\"\\|\") :\n",
        "      return x.split(\"\\|\")[1]\n",
        "df[\"Comments\"] = df[\"ID,Comments\"].apply(split_sep1)"
      ],
      "metadata": {
        "id": "B-llH1Fm4fAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kf8fgswJ5qzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Comments\"]"
      ],
      "metadata": {
        "id": "P8MI7Gpe4qGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ID,Comments\"] = df[\"ID,Comments\"].str.replace('|','__').astype(\"str\")"
      ],
      "metadata": {
        "id": "OZwh6K2caeVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ID,Comments\"] = df[\"ID,Comments\"].str.replace(',','__')"
      ],
      "metadata": {
        "id": "eA99u2UMZG0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ID,Comments\"] = df[\"ID,Comments\"].str.split(\"__\")"
      ],
      "metadata": {
        "id": "9_aHO3MrbvTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(16)"
      ],
      "metadata": {
        "id": "rx-Jf6PgarNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ID\"] = df[\"ID,Comments\"].apply(lambda x : x[0])"
      ],
      "metadata": {
        "id": "-Jw7YodjXY4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ID\"].astype(\"int64\")\n",
        "df[\"ID\"]"
      ],
      "metadata": {
        "id": "3uzEfNIIYiP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Comments\"] = df[\"ID,Comments\"].apply(lambda x : x[1])\n",
        "df[\"Comments\"]"
      ],
      "metadata": {
        "id": "bQUZOQ_4dxdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=\"ID,Comments\",inplace=True)"
      ],
      "metadata": {
        "id": "HTbfAoFzKolR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "UaXDI95pKofp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "a4qjwt4VKocy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "diuzVur66UIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "for i in [\"not\", \"no\"]:\n",
        "        stop_words.remove(i)"
      ],
      "metadata": {
        "id": "R19vv_-Ie_bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning(data):\n",
        "    \n",
        "    #1. Tokenize\n",
        "    text_tokens = word_tokenize(data.replace(\"'\", \"\").lower())\n",
        "    \n",
        "    #2. Remove Puncs and numbers\n",
        "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
        "    \n",
        "    #3. Removing Stopwords\n",
        "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
        "    \n",
        "    #4. lemma\n",
        "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
        "    \n",
        "    #joining\n",
        "    return \" \".join(text_cleaned)"
      ],
      "metadata": {
        "id": "U-EPSM_O6YWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "SYxyXZlo7-gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "max_token = []\n",
        "for sent in df[\"Comments\"]:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    \n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    max_token.append(len(input_ids))\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "metadata": {
        "id": "9_JYwZGw8Dqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(np.array(max_token) <= 162) / len(max_token)"
      ],
      "metadata": {
        "id": "bczNEKqy8D_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "id": "bcSfZy0MF4tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "from sklearn.decomposition import PCA\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "#from utils import display_pca_scatterplot\n",
        "import json\n",
        "import pandas as pd\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.tr import Turkish\n",
        "import numpy as np\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ],
      "metadata": {
        "id": "BXLPOKa3Dvtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = \\\n",
        "              np.split(df[\"Comments\"].sample(frac=1, random_state=42), \n",
        "                       [int(.9*len(df[\"Comments\"]))])"
      ],
      "metadata": {
        "id": "p3g7PnlFET12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {'negative':0, 'neutral':1, 'positive':2}\n",
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "for element in tqdm(train):\n",
        "    X_train.append(element['sentence'])\n",
        "    y_train.append(mapping[element['value']])\n",
        "for element in tqdm(test):\n",
        "    X_test.append(element['sentence'])\n",
        "    y_test.append(mapping[element['value']])"
      ],
      "metadata": {
        "id": "Qgw8qgCs8EKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9JTiCNiA8EWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "av_sROr98EhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qkS7p2e8Erz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CQx_bJK8EzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xC7G7zbS8E5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aVINlZT07-p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHDgb4dy6YCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50S1z6Ja6X5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDvsB0oN6Xlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fmFi5wYX6Xc_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}